{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7badfc3c-4be2-4cb9-9a07-0e46e80fd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1648220c-7889-40d1-b4c5-171a45286e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 6])\n",
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 实例化LSTM对象\n",
    "# 第一个参数: input_size(输入张量维度)\n",
    "# 第二个参数: hidden_size(隐藏层维度)\n",
    "# 第三个参数: num_layer(隐层层的层数)\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 6\n",
    "num_layer = 2\n",
    "seq_len = 4\n",
    "batch_size = 3\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layer, bidirectional=False)\n",
    "# 初始化张量x\n",
    "# 第一个参数: seq_len(输入序列长度)\n",
    "# 第二个参数: batch_size(批次样本数)\n",
    "# 第三个参数: input_size(输入张量维度)\n",
    "input1 = torch.randn(seq_len, batch_size, input_size)\n",
    "# 初始化隐层张量h0,细胞状态c0\n",
    "# 第一个参数: num_layer*方向数(隐层层的层数*方向数)\n",
    "# 第二个参数: batch_size(批次样本数)\n",
    "# 第三个参数: hidden_size(隐藏层维度)\n",
    "h0 = torch.randn(num_layer, batch_size, hidden_size)\n",
    "c0 = torch.randn(num_layer, batch_size, hidden_size)\n",
    "\n",
    "outputs, (hn, cn) = lstm(input1, (h0, c0))\n",
    "print(outputs.shape)\n",
    "print(hn.shape)\n",
    "print(cn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6aaaf-e484-447c-a1de-0ab6033b12df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd7d31f-fed8-43e6-aa74-1b08cc36a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "# input_size = 10  # 输入维度\n",
    "# hidden_size = 20  # 隐藏层维度\n",
    "# layer_size = 2    # LSTM层数\n",
    "# bidirectional = True  # 双向LSTM\n",
    "\n",
    "# lstm = MyLSTM(input_size, hidden_size, layer_size, bidirectional)\n",
    "\n",
    "# # 模拟输入：假设序列长度为5，batch size为3，输入维度为10\n",
    "# seq_len = 5\n",
    "# batch_size = 3\n",
    "# inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "\n",
    "# # 初始化隐藏状态和记忆单元，双向LSTM每层有两个方向\n",
    "# h_0 = [torch.zeros(batch_size, hidden_size) for _ in range(layer_size * (2 if bidirectional else 1))]\n",
    "# c_0 = [torch.zeros(batch_size, hidden_size) for _ in range(layer_size * (2 if bidirectional else 1))]\n",
    "\n",
    "# # 前向传播\n",
    "# output, (h_n, c_n) = lstm(inputs, (h_0, c_0))\n",
    "\n",
    "# print(\"输出：\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465e198f-55af-4eff-af47-6378c24f97ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05880117-9986-4612-9a97-d01a94de3d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac77ca83-d0db-44bf-8994-563fedd98ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "# input_size = 10  # 输入维度\n",
    "# hidden_size = 20  # 隐藏层维度\n",
    "# layer_size = 2    # LSTM层数\n",
    "\n",
    "# input_size = 5\n",
    "# hidden_size = 6\n",
    "# layer_size = 2\n",
    "# seq_len = 1\n",
    "# batch_size = 3\n",
    "\n",
    "# bidirectional = True  # 双向LSTM\n",
    "\n",
    "# lstm = MyLSTM(input_size, hidden_size, layer_size, bidirectional)\n",
    "\n",
    "# # 模拟输入：假设序列长度为5，batch size为3，输入维度为10\n",
    "# # seq_len = 5\n",
    "# batch_size = 3\n",
    "# inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "\n",
    "# # 初始化隐藏状态和记忆单元，双向LSTM每层有两个方向\n",
    "# h_0 = [torch.zeros(batch_size, hidden_size) for _ in range(layer_size * (2 if bidirectional else 1))]\n",
    "# c_0 = [torch.zeros(batch_size, hidden_size) for _ in range(layer_size * (2 if bidirectional else 1))]\n",
    "\n",
    "# # 前向传播\n",
    "# output, (h_n, c_n) = lstm(inputs, (h_0, c_0))\n",
    "\n",
    "# print(\"输出：\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e17cb-9ab0-4101-8c5d-841bb8e968e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee21a6b1-4aac-496f-9aaa-8e078a379d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size=1, bidirectional=False):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_size = layer_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # 初始化每一层的LSTM\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "\n",
    "        for layer in range(layer_size):\n",
    "            if layer == 0:\n",
    "                in_size = input_size\n",
    "            else:\n",
    "                in_size = hidden_size * self.num_directions\n",
    "            \n",
    "            # 添加一层LSTM\n",
    "            self.lstm_layers.append(self._create_lstm_layer(in_size, layer))\n",
    "\n",
    "    def _create_lstm_layer(self, input_size, layer_idx):\n",
    "        # 一个LSTM层中包含正向和（可能有）反向\n",
    "        layer = nn.ModuleDict()\n",
    "        # 给每一层的forward和backward加上唯一的层索引\n",
    "        layer[f'forward_{layer_idx}'] = self._create_lstm_unit(input_size)\n",
    "        if self.bidirectional:\n",
    "            layer[f'backward_{layer_idx}'] = self._create_lstm_unit(input_size)\n",
    "        return layer\n",
    "\n",
    "    def _create_lstm_unit(self, input_size):\n",
    "        # LSTM 单元的各个门\n",
    "        unit = nn.ModuleDict({\n",
    "            'W_f': nn.Linear(input_size + self.hidden_size, self.hidden_size),\n",
    "            'W_i': nn.Linear(input_size + self.hidden_size, self.hidden_size),\n",
    "            'W_o': nn.Linear(input_size + self.hidden_size, self.hidden_size),\n",
    "            'W_c': nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        })\n",
    "        return unit\n",
    "\n",
    "    def forward(self, x, h0=None, C0=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.layer_size * self.num_directions, batch_size, self.hidden_size).to(x.device)\n",
    "        if C0 is None:\n",
    "            C0 = torch.zeros(self.layer_size * self.num_directions, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        h_n, C_n = [], []\n",
    "        \n",
    "        for layer_idx, lstm_layer in enumerate(self.lstm_layers):\n",
    "            h_layer = []\n",
    "            C_layer = []\n",
    "            output_fwd, output_bwd = [], []\n",
    "\n",
    "            # 正向传播\n",
    "            h_fwd = h0[layer_idx * self.num_directions]\n",
    "            C_fwd = C0[layer_idx * self.num_directions]\n",
    "            for t in range(seq_len):\n",
    "                h_fwd, C_fwd = self._lstm_step(lstm_layer[f'forward_{layer_idx}'], x[:, t, :], h_fwd, C_fwd)\n",
    "                output_fwd.append(h_fwd.unsqueeze(1))\n",
    "\n",
    "            output_fwd = torch.cat(output_fwd, dim=1)\n",
    "\n",
    "            if self.bidirectional:\n",
    "                # 反向传播\n",
    "                h_bwd = h0[layer_idx * self.num_directions + 1]\n",
    "                C_bwd = C0[layer_idx * self.num_directions + 1]\n",
    "                for t in reversed(range(seq_len)):\n",
    "                    h_bwd, C_bwd = self._lstm_step(lstm_layer[f'backward_{layer_idx}'], x[:, t, :], h_bwd, C_bwd)\n",
    "                    output_bwd.append(h_bwd.unsqueeze(1))\n",
    "                \n",
    "                output_bwd = torch.cat(output_bwd, dim=1)\n",
    "                output = torch.cat([output_fwd, output_bwd], dim=2)  # 双向拼接\n",
    "            else:\n",
    "                output = output_fwd\n",
    "\n",
    "            # 更新输入 x 为输出\n",
    "            x = output\n",
    "            h_layer.append(h_fwd)\n",
    "            C_layer.append(C_fwd)\n",
    "\n",
    "            if self.bidirectional:\n",
    "                h_layer.append(h_bwd)\n",
    "                C_layer.append(C_bwd)\n",
    "\n",
    "            h_n.append(torch.stack(h_layer))\n",
    "            C_n.append(torch.stack(C_layer))\n",
    "\n",
    "        h_n = torch.stack(h_n, dim=0)  # 堆叠隐藏状态\n",
    "        C_n = torch.stack(C_n, dim=0)  # 堆叠细胞状态\n",
    "        \n",
    "        return output, (h_n, C_n)\n",
    "\n",
    "    def _lstm_step(self, unit, x_t, h_prev, C_prev):\n",
    "        concat = torch.cat((h_prev, x_t), dim=1)\n",
    "\n",
    "        f_t = torch.sigmoid(unit['W_f'](concat))\n",
    "        i_t = torch.sigmoid(unit['W_i'](concat))\n",
    "        C_tilde = torch.tanh(unit['W_c'](concat))\n",
    "        o_t = torch.sigmoid(unit['W_o'](concat))\n",
    "\n",
    "        C_t = f_t * C_prev + i_t * C_tilde\n",
    "        h_t = o_t * torch.tanh(C_t)\n",
    "\n",
    "        return h_t, C_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be423b9-2d10-476a-b0cd-f952a23b0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([3, 4, 6])\n",
      "h_n shape: torch.Size([2, 1, 3, 6])\n",
      "C_n shape: torch.Size([2, 1, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 测试MyLSTM\n",
    "input_size = 5\n",
    "hidden_size = 6\n",
    "layer_size = 2\n",
    "seq_len = 4\n",
    "batch_size = 3\n",
    "bidirectional = False\n",
    "\n",
    "# 假设输入为 (batch_size, seq_len, input_size)\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # batch_size=4, seq_len=6, input_size=5\n",
    "\n",
    "# 实例化自定义 LSTM\n",
    "my_lstm = MyLSTM(input_size, hidden_size, layer_size, bidirectional)\n",
    "\n",
    "# 执行前向传播\n",
    "output, (h_n, C_n) = my_lstm(x)\n",
    "print(\"output shape:\", output.shape)  # (batch_size, seq_len, hidden_size * num_directions)\n",
    "print(\"h_n shape:\", h_n.shape)  # (layer_size * num_directions, batch_size, hidden_size)\n",
    "print(\"C_n shape:\", C_n.shape)  # (layer_size * num_directions, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c467e12-2de8-4a79-acb8-1748e770419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 6\n",
    "num_layer = 2\n",
    "seq_len = 1\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc1e8f-3ea1-4257-8154-27c127b09ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1913b1d-261c-4b82-8a34-a169e2b6de77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf295ce-cc1f-450e-b604-9fdfc0e4cbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08335e0f-1584-4186-94f6-95127603759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前报错，暂时没有找到原因\n",
    "class My_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer, bidirectional=False):\n",
    "        super(My_LSTM, self).__init__()\n",
    "        # 定义参数\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "\n",
    "        # 构建子模块\n",
    "        self.lstm_layer = nn.ModuleList()\n",
    "        for layer in range(self.num_layer):\n",
    "            # 修正接口，对于非第一层，输入的接口大小与前一层有关，且大小为input_size*self.num_directions\n",
    "            if layer == 0:\n",
    "                in_size = input_size\n",
    "            else:\n",
    "                in_size = input_size*self.num_directions\n",
    "            # 添加lstm_layer层\n",
    "            self.lstm_layer.append(self._create_lstm_layer(in_size, layer))\n",
    "\n",
    "    # 创建lstm层--根据传入数据的特征维度和创建对应层的序号\n",
    "    def _create_lstm_layer(self, input_size, layer_idx):\n",
    "        # 创建双向lstm层--单双向为了方便调用，采用键值对方式构建\n",
    "        layer = nn.ModuleDict()\n",
    "        layer[f\"forward_{layer_idx}\"] = self._create_lstm_unit(input_size)\n",
    "        if self.bidirectional:\n",
    "            # 双向\n",
    "            layer[f\"backward_{layer_idx}\"] = self._create_lstm_unit(input_size)\n",
    "        return layer\n",
    "\n",
    "    def _create_lstm_unit(self, input_size):\n",
    "        # 创建lstm层的参数\n",
    "        unit = nn.ModuleDict({\n",
    "            \"w_f\": nn.Linear(input_size + self.hidden_size, self.hidden_size),\n",
    "            \"w_i\": nn.Linear(input_size + self.hidden_size, self.hidden_size),\n",
    "            \"w_c\": nn.Linear(input_size + self.hidden_size, self.hidden_size),\n",
    "            \"w_o\": nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        })\n",
    "        return unit\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, inputs, h_0=None, c_0=None):\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len = inputs.size(1)\n",
    "\n",
    "        if h_0 is None:\n",
    "            h_0 = torch.zeros(self.num_layer*self.num_directions, batch_size, self.hidden_size)\n",
    "        if c_0 is None:\n",
    "            c_0 = torch.zeros(self.num_layer*self.num_directions, batch_size, self.hidden_size)\n",
    "\n",
    "        # 记录隐藏状态和细胞状态\n",
    "        h_n, c_n = [], []\n",
    "        # 先对层进行循环，在每一层中对时间片循环\n",
    "        # 对层循环的过程中，按照键值对方式取层模块\n",
    "        for layer_idx, lstm_layer in enumerate(self.lstm_layer):\n",
    "            h_layer = []\n",
    "            c_layer = []\n",
    "            output_fwd, output_bwd = [], []\n",
    "\n",
    "            # 正向传播\n",
    "            h_fwd = h_0[layer_idx*self.num_directions]\n",
    "            c_fwd = c_0[layer_idx*self.num_directions]\n",
    "            for t in range(seq_len):\n",
    "                h_fwd, c_fwd = self._lstm_step(lstm_layer[f\"forward_{layer_idx}\"], inputs[:, t, :], h_fwd, c_fwd)\n",
    "                output_fwd.append(h_fwd.unsqueeze(1))\n",
    "            output_fwd = torch.cat(output_fwd, dim=1)\n",
    "\n",
    "            if self.bidirectional:\n",
    "                # 如果是双向的，那么准备计算反向过程\n",
    "                h_bwd = h_0[layer_idx * self.num_directions + 1]\n",
    "                c_bwd = c_0[layer_idx * self.num_directions + 1]\n",
    "                for t in reversed(range(seq_len)):\n",
    "                    h_bwd, c_bwd = self._lstm_step(lstm_layer[f\"backward_{layer_idx}\"], inputs[:, t, :], h_bwd, c_bwd)\n",
    "                    output_bwd.append(h_bwd.unsqueeze(1))\n",
    "                output_bwd = torch.cat(output_bwd, dim=1)\n",
    "                # 对output进行双向拼接\n",
    "                output = torch.cat([output_fwd, output_bwd], dim=2)\n",
    "            else:\n",
    "                output = output_fwd\n",
    "\n",
    "            # 进入下一层，对inputs进行更新\n",
    "            inputs = output\n",
    "            h_layer.append(h_fwd)\n",
    "            c_layer.append(c_fwd)\n",
    "            if self.bidirectional:\n",
    "                h_layer.append(h_bwd)\n",
    "                c_layer.append(c_bwd)\n",
    "            h_n.append(torch.stack(h_layer))\n",
    "            c_n.append(torch.stack(c_layer))\n",
    "        h_n = torch.stack(h_n, dim=0)\n",
    "        c_n = torch.stack(c_n, dim=0)\n",
    "        return output, (h_n, c_n)\n",
    "\n",
    "\n",
    "    def _lstm_step(self, unit, inputs, h_pre, c_pre):\n",
    "        concat = torch.cat((h_pre, inputs), dim=1)\n",
    "        f_t = torch.sigmoid(unit[\"w_f\"](concat))\n",
    "        i_t = torch.sigmoid(unit[\"w_i\"](concat))\n",
    "        c_t_title = torch.tanh(unit[\"w_c\"](concat))\n",
    "        o_t = torch.sigmoid(unit[\"w_o\"](concat))\n",
    "        \n",
    "        c_t = f_t * c_pre + i_t * c_t_title\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b969787-c446-456a-ab48-c54ee8ece628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试My_LSTM\n",
    "input_size = 5\n",
    "hidden_size = 6\n",
    "layer_size = 2\n",
    "seq_len = 4\n",
    "batch_size = 3\n",
    "bidirectional = False\n",
    "\n",
    "# 假设输入为 (batch_size, seq_len, input_size)\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # batch_size=4, seq_len=6, input_size=5\n",
    "\n",
    "# 实例化自定义 LSTM\n",
    "my_lstm = My_LSTM(input_size, hidden_size, layer_size, bidirectional)\n",
    "\n",
    "# 执行前向传播\n",
    "output, (h_n, C_n) = my_lstm(x)\n",
    "print(\"output shape:\", output.shape)  # (batch_size, seq_len, hidden_size * num_directions)\n",
    "print(\"h_n shape:\", h_n.shape)  # (layer_size * num_directions, batch_size, hidden_size)\n",
    "print(\"C_n shape:\", C_n.shape)  # (layer_size * num_directions, batch_size, hidden_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdsd_torch",
   "language": "python",
   "name": "sdsd_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
