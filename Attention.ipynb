{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cb45c9-0560-49b5-b054-ac1eefcfbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e74f23-155f-4651-982f-144dc7665841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1.shape: torch.Size([10, 3, 4])\n",
      "mat2.shape: torch.Size([10, 4, 5])\n",
      "res.shape: torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(10, 3, 4)\n",
    "mat2 = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(mat1, mat2)\n",
    "print(f\"mat1.shape: {mat1.shape}\")\n",
    "print(f\"mat2.shape: {mat2.shape}\")\n",
    "print(f\"res.shape: {res.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52501093-615b-4da9-8bc3-371924c2cd97",
   "metadata": {},
   "source": [
    "# 注意力机制(规则一)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83468ea6-2ed2-469a-8dae-da9d2186d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size1, value_size2, output_size):\n",
    "        # query_size 代表Q的最后一个维度, key_size 代表k最后一个维度\n",
    "        # V 的尺寸(1, value_size1, value_size2)\n",
    "        # output_size 输出的最后一个维度\n",
    "        super(Attn, self).__init__()\n",
    "        self.query_size = query_size\n",
    "        self.key_size = key_size\n",
    "        self.value_size1 = value_size1\n",
    "        self.value_size2 = value_size2\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # 注意力机制的线性层\n",
    "        self.attn = nn.Linear(self.query_size + self.key_size, self.value_size1)\n",
    "        # 注意力机制实现的第三步线性层\n",
    "        self.attn_combin = nn.Linear(self.query_size + self.value_size2, self.output_size)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # 假定输入的Q、K、V都是三维\n",
    "        # 第一步, 将Q、K纵轴拼接-->线性变换-->softmax激活====>注意力向量\n",
    "        attn_weight = F.softmax(self.attn(torch.cat((Q[0], K[0]), 1)), dim=1)\n",
    "\n",
    "        # 将注意力矩阵与V张量乘法\n",
    "        attn_applied = torch.bmm(attn_weight.unsqueeze(0), V)\n",
    "\n",
    "        # 取Q[0]降维，再次和上面结果拼接\n",
    "        output = torch.cat((Q[0], attn_applied[0]), 1)\n",
    "\n",
    "        # 第三步, 将上面输出进行线性变换，然后扩展维度到三维\n",
    "        output = self.attn_combin(output).unsqueeze(0)\n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da139e14-1794-4dc5-84f7-e433218bc0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64])\n",
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "query_size = 32\n",
    "key_size = 32\n",
    "value_size1 = 32\n",
    "value_size2 = 64\n",
    "output_size = 64\n",
    "attn = Attn(query_size, key_size, value_size1, value_size2, output_size)\n",
    "Q = torch.randn(1, 1, 32)\n",
    "K = torch.randn(1, 1, 32)\n",
    "V = torch.randn(1, 32, 64)\n",
    "output = attn(Q, K, V)\n",
    "print(output[0].size())\n",
    "print(output[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39121e06-7403-4e62-9a3c-663e528f8765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdsd_torch",
   "language": "python",
   "name": "sdsd_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
