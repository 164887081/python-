{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e22ebc-d296-4ef5-8a08-0a0d2e903363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36529acb-4bce-4d26-9748-dd4df8896bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 12])\n",
      "torch.Size([4, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 实例化对象\n",
    "# 第一个参数 input_size(输入张量x维度)\n",
    "# 第二个参数 hidden_size(隐层的维度,隐藏神经元数量)\n",
    "# 第三个参数 num_layers(隐藏层层数)\n",
    "input_size = 5\n",
    "hidden_size = 6\n",
    "num_layers = 2\n",
    "seq_len = 1\n",
    "batch_size = 3\n",
    "gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True)\n",
    "\n",
    "# 初始化张量input1\n",
    "# 第一个参数: seq_len(序列长度)\n",
    "# 第二个参数: batch_size(批次样本数)\n",
    "# 第三个参数: input_size(输入张量x维度)\n",
    "input1 = torch.randn(seq_len, batch_size, input_size)\n",
    "# 初始化隐层 h0\n",
    "# 第一个参数 num_layers*num_directions(隐藏层层数*方向数)\n",
    "# 第二个参数 batch_size(批次样本数)\n",
    "# 第三个参数 hidden_size(隐层的维度,隐藏神经元数量)\n",
    "h0 = torch.randn(num_layers*2, batch_size, hidden_size)\n",
    "\n",
    "output, hn = gru(input1, h0)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c574ad-83e0-48a0-be98-34ea99a4ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92764d69-4ad1-44f8-8baa-aa9e3d45e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CustomGRU(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "#         super(CustomGRU, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "#         # 定义每一层的参数\n",
    "#         self.gru_cells = nn.ModuleList()\n",
    "#         for layer in range(num_layers):\n",
    "#             input_dim = input_size if layer == 0 else hidden_size * self.num_directions\n",
    "#             self.gru_cells.append(GRUCell(input_dim, hidden_size))\n",
    "\n",
    "#     def forward(self, x, h_0=None):\n",
    "#         seq_len, batch_size, _ = x.size()\n",
    "\n",
    "#         # 初始化隐藏状态\n",
    "#         if h_0 is None:\n",
    "#             h_0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "#         h_n = []\n",
    "#         for layer in range(self.num_layers):\n",
    "#             # 只提取对应层的隐藏状态，避免 h 维度不匹配，因此添加了squeeze(0)\n",
    "#             h = h_0[layer * self.num_directions: (layer + 1) * self.num_directions].squeeze(0)\n",
    "#             outputs = []\n",
    "#             for t in range(seq_len):\n",
    "#                 h = self.gru_cells[layer](x[t], h)\n",
    "#                 outputs.append(h)\n",
    "#             x = torch.stack(outputs, dim=0)  # 叠加时间步\n",
    "#             h_n.append(h)\n",
    "        \n",
    "#         # 返回所有层的最后一个隐藏状态\n",
    "#         return x, torch.stack(h_n, dim=0)\n",
    "\n",
    "# class GRUCell(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(GRUCell, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         # 定义 GRU 的权重矩阵\n",
    "#         self.W_z = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "#         self.W_r = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "#         self.W_h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "#     def forward(self, x, h):\n",
    "#         # 拼接输入和隐藏状态\n",
    "#         combined = torch.cat([x, h], dim=-1)\n",
    "\n",
    "#         # 计算更新门和重置门\n",
    "#         z_t = torch.sigmoid(self.W_z(combined))\n",
    "#         r_t = torch.sigmoid(self.W_r(combined))\n",
    "\n",
    "#         # 计算候选隐藏状态\n",
    "#         combined_r = torch.cat([x, r_t * h], dim=-1)\n",
    "#         h_tilde = torch.tanh(self.W_h(combined_r))\n",
    "\n",
    "#         # 计算新的隐藏状态\n",
    "#         h_new = (1 - z_t) * h + z_t * h_tilde\n",
    "\n",
    "#         return h_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9834fc-02a5-4932-846e-dd20030f22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现双向计算\n",
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # 定义每一层的 GRUCell\n",
    "        self.gru_cells = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            input_dim = input_size if layer == 0 else hidden_size * self.num_directions\n",
    "            self.gru_cells.append(GRUCell(input_dim, hidden_size))\n",
    "            if bidirectional:\n",
    "                self.gru_cells.append(GRUCell(input_dim, hidden_size))  # 反向的GRUCell\n",
    "\n",
    "    def forward(self, x, h_0=None):\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        # 初始化隐藏状态\n",
    "        if h_0 is None:\n",
    "            h_0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        h_n = []\n",
    "        for layer in range(self.num_layers):\n",
    "            # 正向计算\n",
    "            h_fwd = h_0[layer * self.num_directions]  # 正向的隐藏状态\n",
    "            outputs_fwd = []\n",
    "            for t in range(seq_len):\n",
    "                h_fwd = self.gru_cells[layer * self.num_directions](x[t], h_fwd)\n",
    "                outputs_fwd.append(h_fwd)\n",
    "            outputs_fwd = torch.stack(outputs_fwd, dim=0)\n",
    "\n",
    "            if self.bidirectional:\n",
    "                # 反向计算\n",
    "                h_bwd = h_0[layer * self.num_directions + 1]  # 反向的隐藏状态\n",
    "                outputs_bwd = []\n",
    "                for t in reversed(range(seq_len)):\n",
    "                    h_bwd = self.gru_cells[layer * self.num_directions + 1](x[t], h_bwd)\n",
    "                    outputs_bwd.append(h_bwd)\n",
    "                outputs_bwd = torch.stack(outputs_bwd[::-1], dim=0)  # 调整为正向顺序\n",
    "\n",
    "                # 拼接正向和反向的输出\n",
    "                x = torch.cat([outputs_fwd, outputs_bwd], dim=-1)\n",
    "            else:\n",
    "                x = outputs_fwd\n",
    "\n",
    "            h_n.append(h_fwd if not self.bidirectional else torch.cat([h_fwd, h_bwd], dim=-1))\n",
    "\n",
    "        # 返回最后一层的所有时间步的输出，以及所有层的最后一个隐藏状态\n",
    "        return x, torch.stack(h_n, dim=0)\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 定义 GRU 的权重矩阵\n",
    "        self.W_z = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_r = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # 拼接输入和隐藏状态\n",
    "        combined = torch.cat([x, h], dim=-1)\n",
    "\n",
    "        # 计算更新门和重置门\n",
    "        z_t = torch.sigmoid(self.W_z(combined))\n",
    "        r_t = torch.sigmoid(self.W_r(combined))\n",
    "\n",
    "        # 计算候选隐藏状态\n",
    "        combined_r = torch.cat([x, r_t * h], dim=-1)\n",
    "        h_tilde = torch.tanh(self.W_h(combined_r))\n",
    "\n",
    "        # 计算新的隐藏状态\n",
    "        h_new = (1 - z_t) * h + z_t * h_tilde\n",
    "\n",
    "        return h_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a44bbf4-f8b0-44fe-b947-6c6f8ebfdea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 12])\n",
      "torch.Size([2, 3, 12])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 6\n",
    "num_layers = 2\n",
    "seq_len = 1\n",
    "batch_size = 3\n",
    "gru = CustomGRU(input_size, hidden_size, num_layers, bidirectional=True)\n",
    "input1 = torch.randn(seq_len, batch_size, input_size)\n",
    "output, hn = gru(input1)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140499f-bed2-471d-ac4a-8b42006b872c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0811dc-270b-42c7-8ac3-3f2cd331099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单向没有问题，双向错误\n",
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=False):\n",
    "        super(MyGRU, self).__init__()\n",
    "        # 定义网络参数\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "\n",
    "        # 定义每一层的结构\n",
    "        self.gru_layers = nn.ModuleList()\n",
    "        for layer in range(self.num_layers):\n",
    "            in_size = self.input_size if layer == 0 else self.hidden_size * self.num_directions\n",
    "            self.gru_layers.append(GRU_Cell(in_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, inputs, h0=None):\n",
    "        seq_len, batch_size, _ = inputs.size()\n",
    "\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "        \n",
    "        h_n = []\n",
    "        for layer in range(self.num_layers):\n",
    "            # 只提取对应层的隐藏状态\n",
    "            # 正向计算\n",
    "            h_fwd = h0[layer*self.num_directions] # 正向的隐藏状态\n",
    "            outputs_fwd = []\n",
    "            for t in range(seq_len):\n",
    "                h_fwd = self.gru_layers[layer * self.num_directions](inputs[t], h_fwd)\n",
    "                outputs_fwd.append(h_fwd)\n",
    "            outputs_fwd = torch.stack(outputs_fwd, dim=0)\n",
    "\n",
    "            if self.bidirectional:\n",
    "                # 计算反向\n",
    "                # 获取反向隐藏状态\n",
    "                h_bwd = h0[layer * self.num_directions + 1]\n",
    "                outputs_bwd = []\n",
    "                for t in reversed(range(seq_len)):\n",
    "                    h_bwd = self.gru_layers[layer * self.num_directions + 1](inputs[t], h_bwd)\n",
    "                    outputs_bwd.append(h_bwd)\n",
    "                outputs_bwd = torch.stack(outputs_bwd[:, :, -1], dim=0)\n",
    "\n",
    "                # 拼接正向和反向\n",
    "                inputs = torch.cat([outputs_fwd, outputs_bwd], dim=-1)\n",
    "            else:\n",
    "                inputs = outputs_fwd\n",
    "\n",
    "            h_n.append(h_fwd if not self.bidirectional else torch.cat([h_fwd, h_bwd], dim=-1))\n",
    "\n",
    "        return inputs, torch.stack(h_n, dim=0)\n",
    "                    \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "class GRU_Cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU_Cell, self).__init__()\n",
    "        # 在单层内的结构定义\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.w_z = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "        self.w_r = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "        self.w_t = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "\n",
    "    \n",
    "    # 在单层内前向传播\n",
    "    def forward(self, inputs, h_pre):\n",
    "        concat = torch.cat([inputs, h_pre], dim=-1)\n",
    "\n",
    "        # 计算重置门和更新门\n",
    "        z_t = torch.sigmoid(self.w_z(concat))\n",
    "        r_t = torch.sigmoid(self.w_r(concat))\n",
    "        # 计算候选隐藏状态\n",
    "        rh = r_t * h_pre\n",
    "        rx = torch.cat([inputs, rh], dim=-1)\n",
    "        h_t_title = torch.tanh(self.w_t(rx))\n",
    "        # 计算新的隐藏状态\n",
    "        h_t = (1 - z_t) * h_pre + z_t * h_t_title\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "759ebc18-6670-4197-9371-8912b3161e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 6\n",
    "num_layers = 2\n",
    "seq_len = 1\n",
    "batch_size = 3\n",
    "gru = MyGRU(input_size, hidden_size, num_layers, bidirectional=False)\n",
    "input1 = torch.randn(seq_len*2, batch_size, input_size)\n",
    "output, hn = gru(input1)\n",
    "print(output.shape)\n",
    "print(hn.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdsd_torch",
   "language": "python",
   "name": "sdsd_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
